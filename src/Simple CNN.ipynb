{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c091cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "File to train and output a simple custom CNN baseline for HAM10k lesion classification\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Constants\n",
    "SEED = 4\n",
    "DATA_DIR = \"data/ham10k_data/\"\n",
    "PATH_TO_METADATA_FILE = os.path.join(DATA_DIR, \"HAM10000_metadata.csv\")\n",
    "PATH_TO_IMAGES = os.path.join(DATA_DIR, \"HAM10000_images/\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10  \n",
    "NUM_CLASSES = 7  # number of diagnosis classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class HAMDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.loc[idx, \"image_id\"]\n",
    "        label = int(self.df.loc[idx, \"dx\"])\n",
    "        path = os.path.join(self.img_dir, img_id + \".jpg\")\n",
    "        image = read_image(path)  # returns C×H×W\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 3 Conv layers: 3->32, 32->64, 64->128\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # single pooling + dropout\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        # classifier\n",
    "        # after conv: input size remains 224, pooling halves to 112\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 112 * 112, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total * 100\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss = val_loss / val_total\n",
    "        val_acc = val_correct / val_total * 100\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "              f\"Train: loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "              f\"Val:   loss={val_loss:.4f}, acc={val_acc:.2f}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # read in metadata\n",
    "    metadata_df = pd.read_csv(PATH_TO_METADATA_FILE)\n",
    "\n",
    "    # map label to numeric\n",
    "    label_map = {label: i for i, label in enumerate(metadata_df[\"dx\"].unique())}\n",
    "    print(label_map)\n",
    "    metadata_df[\"dx\"] = metadata_df[\"dx\"].map(label_map)\n",
    "\n",
    "    # split: 70% train, 30% temp\n",
    "    train_df, temp_df = train_test_split(\n",
    "        metadata_df, test_size=0.3, shuffle=True, random_state=SEED\n",
    "    )\n",
    "    # split temp into 50% test, 50% val (each 15% overall)\n",
    "    test_df, val_df = train_test_split(\n",
    "        temp_df, test_size=0.5, shuffle=True, random_state=SEED\n",
    "    )\n",
    "\n",
    "    # transforms & datasets\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),\n",
    "    ])\n",
    "\n",
    "    train_ds = HAMDataset(train_df, PATH_TO_IMAGES, transform)\n",
    "    val_ds   = HAMDataset(val_df,   PATH_TO_IMAGES, transform)\n",
    "    test_ds  = HAMDataset(test_df,  PATH_TO_IMAGES, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # model, loss, optimizer\n",
    "    model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # train\n",
    "    history = train_model(model, train_loader, val_loader,\n",
    "                          criterion, optimizer, epochs=EPOCHS)\n",
    "\n",
    "    # plot & save\n",
    "    plt.figure(); plt.plot(history[\"train_acc\"], label=\"Train Acc\"); plt.plot(history[\"val_acc\"], label=\"Val Acc\"); plt.legend(); plt.savefig(\"accuracy_curve.png\")\n",
    "    plt.figure(); plt.plot(history[\"train_loss\"], label=\"Train Loss\"); plt.plot(history[\"val_loss\"], label=\"Val Loss\"); plt.legend(); plt.savefig(\"loss_curve.png\")\n",
    "\n",
    "    # test predictions\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    # save preds\n",
    "    filenames = test_df[\"image_id\"].tolist()\n",
    "    out_df = pd.DataFrame({\"img_id\": filenames, \"pred_label\": preds})\n",
    "    out_df.to_csv(\"predictions.csv\", index=False)\n",
    "    print(\"Test predictions saved to predictions.csv\")\n",
    "\n",
    "    # test accuracy\n",
    "    true = test_df[\"dx\"].values\n",
    "    acc = (np.array(preds) == true).mean() * 100\n",
    "    print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
